{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72923f62-8e96-40b2-8d66-39f84ec987aa",
   "metadata": {},
   "source": [
    "# Transfer Learning part 1: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738215b-d974-45a4-9da3-a9592b1d3318",
   "metadata": {},
   "source": [
    "_Transfer leaning is a process of leveraging a  working model's architecture and learning pattern for our own problems_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc79101-caef-4326-86d4-8da45be927fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      2.1.0\n",
      "anyio                        4.10.0\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         25.1.0\n",
      "asttokens                    3.0.0\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.5\n",
      "attrs                        25.4.0\n",
      "babel                        2.16.0\n",
      "beautifulsoup4               4.13.5\n",
      "bleach                       6.2.0\n",
      "brotlicffi                   1.1.0.0\n",
      "certifi                      2023.5.7\n",
      "cffi                         2.0.0\n",
      "charset-normalizer           3.2.0\n",
      "click                        8.1.7\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.1\n",
      "contourpy                    1.3.3\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.16\n",
      "decorator                    5.2.1\n",
      "defusedxml                   0.7.1\n",
      "executing                    2.2.1\n",
      "fastjsonschema               2.20.0\n",
      "flatbuffers                  25.9.23\n",
      "fonttools                    4.60.1\n",
      "gast                         0.5.4\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.62.1\n",
      "h11                          0.16.0\n",
      "h5py                         3.15.1\n",
      "httpcore                     1.0.9\n",
      "httpx                        0.28.1\n",
      "idna                         3.4\n",
      "ipykernel                    6.31.0\n",
      "ipython                      9.7.0\n",
      "ipython_pygments_lexers      1.1.1\n",
      "ipywidgets                   8.1.7\n",
      "jedi                         0.19.2\n",
      "Jinja2                       3.1.6\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.25\n",
      "jsonschema                   4.25.0\n",
      "jsonschema-specifications    2025.9.1\n",
      "jupyter                      1.1.1\n",
      "jupyter_client               8.6.3\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.8.1\n",
      "jupyter-events               0.12.0\n",
      "jupyter-lsp                  2.2.5\n",
      "jupyter_server               2.16.0\n",
      "jupyter_server_terminals     0.5.3\n",
      "jupyterlab                   4.4.7\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.28.0\n",
      "jupyterlab_widgets           3.0.15\n",
      "keras                        3.12.0\n",
      "kiwisolver                   1.4.9\n",
      "libclang                     18.1.1\n",
      "Markdown                     3.6\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.5\n",
      "matplotlib                   3.10.7\n",
      "matplotlib-inline            0.2.1\n",
      "mdurl                        0.1.2\n",
      "mistune                      3.1.2\n",
      "ml-dtypes                    0.4.1\n",
      "namex                        0.0.7\n",
      "nbclient                     0.10.2\n",
      "nbconvert                    7.16.6\n",
      "nbformat                     5.10.4\n",
      "nest-asyncio                 1.6.0\n",
      "nltk                         3.8.1\n",
      "notebook                     7.4.5\n",
      "notebook_shim                0.2.4\n",
      "numpy                        1.26.4\n",
      "opt-einsum                   3.3.0\n",
      "optree                       0.10.0\n",
      "overrides                    7.7.0\n",
      "packaging                    24.0\n",
      "pandas                       2.3.3\n",
      "pandocfilters                1.5.1\n",
      "parso                        0.8.5\n",
      "Pillow                       10.0.0\n",
      "pip                          25.2\n",
      "platformdirs                 4.5.0\n",
      "prometheus_client            0.21.1\n",
      "prompt_toolkit               3.0.52\n",
      "protobuf                     4.25.3\n",
      "psutil                       7.0.0\n",
      "pure_eval                    0.2.3\n",
      "pycparser                    2.23\n",
      "Pygments                     2.17.2\n",
      "pyparsing                    3.2.5\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.9.0.post0\n",
      "python-json-logger           3.2.1\n",
      "pytz                         2025.2\n",
      "pywin32                      311\n",
      "pywinpty                     2.0.15\n",
      "PyYAML                       6.0.2\n",
      "pyzmq                        27.1.0\n",
      "qtconsole                    5.7.0\n",
      "QtPy                         2.4.3\n",
      "referencing                  0.37.0\n",
      "regex                        2023.12.25\n",
      "requests                     2.31.0\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rich                         13.7.1\n",
      "rpds-py                      0.28.0\n",
      "scikit-learn                 1.7.2\n",
      "scipy                        1.16.3\n",
      "seaborn                      0.13.2\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   80.9.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soupsieve                    2.5\n",
      "stack_data                   0.6.3\n",
      "tensorboard                  2.18.0\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.18.0\n",
      "tensorflow-hub               0.16.1\n",
      "tensorflow_intel             2.18.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.4.0\n",
      "terminado                    0.18.1\n",
      "tf_keras                     2.18.0\n",
      "threadpoolctl                3.6.0\n",
      "tinycss2                     1.4.0\n",
      "tornado                      6.5.1\n",
      "tqdm                         4.66.2\n",
      "traitlets                    5.14.3\n",
      "typing_extensions            4.10.0\n",
      "tzdata                       2025.2\n",
      "urllib3                      2.0.4\n",
      "wcwidth                      0.2.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.8.0\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.43.0\n",
      "widgetsnbextension           4.0.14\n",
      "win_inet_pton                1.1.0\n",
      "wrapt                        1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dc80a6-e31c-4f4b-be65-ac2b779d06eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 13 17:49:06 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.83                 Driver Version: 572.83         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   37C    P8              4W /   30W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97816211-9b7d-494d-80a5-07241a09e8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 10_food_classes_10_percent.zip...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\"\n",
    "filename = \"10_food_classes_10_percent.zip\"\n",
    "\n",
    "# Download the file\n",
    "print(f\"Downloading {filename}...\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5fd57a-41bc-4720-a943-18412f9baea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in 10_food_classes_10_percent\n",
      "There are 10 directories and 0 images in 10_food_classes_10_percent\\test\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\chicken_curry\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\chicken_wings\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\fried_rice\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\grilled_salmon\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\hamburger\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\ice_cream\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\pizza\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\ramen\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\steak\n",
      "There are 0 directories and 250 images in 10_food_classes_10_percent\\test\\sushi\n",
      "There are 10 directories and 0 images in 10_food_classes_10_percent\\train\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\chicken_curry\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\chicken_wings\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\fried_rice\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\grilled_salmon\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\hamburger\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\ice_cream\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\pizza\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\ramen\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\steak\n",
      "There are 0 directories and 75 images in 10_food_classes_10_percent\\train\\sushi\n"
     ]
    }
   ],
   "source": [
    "#  how many images in each folder\n",
    "import os\n",
    "\n",
    "# Walk through 1 percent data dict\n",
    "for dirpath, dirname, filename in os.walk(\"10_food_classes_10_percent\"):\n",
    "    print(f\"There are {len(dirname)} directories and {len(filename)} images in {dirpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314c037-3f63-4b5a-bd12-8362c59c8bd7",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6138233-c798-46ed-adce-13f9b6c3359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning data images\n",
      "Found 750 images belonging to 10 classes.\n",
      "testing data images\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# setup inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SHAPE=(224,224)\n",
    "BATCH_SIZE = 32\n",
    "train_dir=\"10_food_classes_10_percent/train\"\n",
    "test_dir=\"10_food_classes_10_percent/test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "print(\"Traning data images\")\n",
    "# Use flow_from_directory to load images from a directory structure\n",
    "train_data_10_percent=train_datagen.flow_from_directory(train_dir,\n",
    "                                                       target_size=IMAGE_SHAPE,# Path to the directory containing subdirectories for each class\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode=\"categorical\")\n",
    "\n",
    "print(\"testing data images\")\n",
    "# Use flow_from_directory to load images from a directory structure\n",
    "train_data_10_percent=train_datagen.flow_from_directory(test_dir,\n",
    "                                                       target_size=IMAGE_SHAPE,# Path to the directory containing subdirectories for each class\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1002f-c3c9-4a24-8b43-30b2f5a9727e",
   "metadata": {},
   "source": [
    "## Setting up callbacks (things to run while uor model train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b75d4-847d-4a84-bdeb-fab4fc4929f7",
   "metadata": {},
   "source": [
    "_creating a function for tf callback because we neet to create a new one for each_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0e395-7c71-4e81-b18f-acd44f1ab16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_tensorflow_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%y/%m/%d_%H:%M:%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f\"saving tensorflow log files to {log_dir}\")\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bad6740b-11d4-40c9-8436-5d0e3145513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from kagglehub) (24.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\updes\\anaconda3\\envs\\myenv\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from kagglehub) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\updes\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Using cached kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b84e5f2-560e-418f-80ac-c2f860d1d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: C:\\Users\\updes\\.cache\\kagglehub\\models\\keras\\efficientnetv2\\keras\\efficientnetv2_b0\\2\n",
      "Path to model files: C:\\Users\\updes\\.cache\\kagglehub\\models\\google\\efficientnet-v2\\tensorFlow2\\imagenet1k-b0-classification\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "imagenet = kagglehub.model_download(\"google/efficientnet-v2/tensorFlow2/imagenet1k-b0-classification\")\n",
    "efficientnetv2_path = kagglehub.model_download(\"keras/efficientnetv2/keras/efficientnetv2_b0\")\n",
    "\n",
    "print(\"Path to model files:\", efficientnetv2_path)\n",
    "print(\"Path to model files:\", imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee36792-ee18-486f-9f9c-e271ffc00b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\updes\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\updes\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d26385d4-095e-42ee-a0d7-f3e20a4afafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_url,num_classes):\n",
    "    feature_extraction_layer = hub.KerasLayer(model_url,\n",
    "                                              trainable=False,\n",
    "                                              name=\"feature_extraction_layer\",\n",
    "                                              input_shape=IMAGE_SHAPE+(3,) #(224, 224, 3, 3)\n",
    "                                             )\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extraction_layer,a\n",
    "        layers.Dense(num_classes,activation=\"softmax\",name=\"output_layer\")\n",
    "    ])\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde2cd7-bfb7-47bd-8837-ed392cc2358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_url,num_classes):\n",
    "    feature_extraction_layer = hub.KerasLayer(model_url,\n",
    "                                              trainable=False,\n",
    "                                              name=\"feature_extraction_layer\",\n",
    "                                             )\n",
    "    # 2. Define the model inputs\n",
    "    inputs = tf.keras.Input(shape=IMAGE_SHAPE+(3,),\n",
    "                            dtype=\"float32\",\n",
    "                            name=\"input_layer\")\n",
    "\n",
    "    # 3. Pass the inputs through the layers\n",
    "    x = feature_extraction_layer(inputs)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "    \n",
    "    # 4. Create the Keras Model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18124f35-b484-4ee3-864c-e7b902f0bbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_SHAPE+(3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f1160-ad3d-4208-b8bc-7cd47313df9a",
   "metadata": {},
   "source": [
    "### creating and testing ResNet Tensorflow hub Features Extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d9a70fe-f036-4130-aed9-be2be69de99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'feature_extraction_layer' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'feature_extraction_layer' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=input_layer>\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m efficientnet_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m imagenet_model = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnet_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_10_percent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_url, num_classes)\u001b[39m\n\u001b[32m      7\u001b[39m inputs = tf.keras.Input(shape=IMAGE_SHAPE+(\u001b[32m3\u001b[39m,),\n\u001b[32m      8\u001b[39m                         dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m                         name=\u001b[33m\"\u001b[39m\u001b[33minput_layer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 3. Pass the inputs through the layers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m x = \u001b[43mfeature_extraction_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m outputs = layers.Dense(num_classes, activation=\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33moutput_layer\u001b[39m\u001b[33m\"\u001b[39m)(x)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 4. Create the Keras Model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:250\u001b[39m, in \u001b[36mKerasLayer.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m    247\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[32m    249\u001b[39m     training = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m   result = \u001b[43msmart_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43msmart_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_key:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:252\u001b[39m, in \u001b[36mKerasLayer.call.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    247\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[32m    249\u001b[39m     training = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    250\u001b[39m   result = smart_cond.smart_cond(training,\n\u001b[32m    251\u001b[39m                                  \u001b[38;5;28;01mlambda\u001b[39;00m: f(training=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m                                  \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_key:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:583\u001b[39m, in \u001b[36mcanonicalize_to_monomorphic\u001b[39m\u001b[34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[39m\n\u001b[32m    577\u001b[39m       parameters.append(\n\u001b[32m    578\u001b[39m           _make_validated_mono_param(kwarg_name, arg[kwarg_name],\n\u001b[32m    579\u001b[39m                                      Parameter.KEYWORD_ONLY, type_context,\n\u001b[32m    580\u001b[39m                                      poly_parameter.type_constraint))\n\u001b[32m    581\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    582\u001b[39m     parameters.append(\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m         \u001b[43m_make_validated_mono_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly_parameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mtype_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mpoly_parameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m FunctionType(parameters, capture_types), type_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:522\u001b[39m, in \u001b[36m_make_validated_mono_param\u001b[39m\u001b[34m(name, value, kind, type_context, poly_type)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_make_validated_mono_param\u001b[39m(\n\u001b[32m    519\u001b[39m     name, value, kind, type_context, poly_type\n\u001b[32m    520\u001b[39m ) -> Parameter:\n\u001b[32m    521\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m   mono_type = \u001b[43mtrace_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m poly_type \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mono_type.is_subtype_of(poly_type):\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` was expected to be of type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoly_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmono_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tensorflow\\core\\function\\trace_type\\trace_type_builder.py:185\u001b[39m, in \u001b[36mfrom_value\u001b[39m\u001b[34m(value, context)\u001b[39m\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m default_types.Attrs.from_type_and_attributes(\n\u001b[32m    179\u001b[39m       \u001b[38;5;28mtype\u001b[39m(value),\n\u001b[32m    180\u001b[39m       \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    181\u001b[39m           from_value(\u001b[38;5;28mgetattr\u001b[39m(value, a.name), context)\n\u001b[32m    182\u001b[39m           \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m value.__attrs_attrs__))\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m util.is_np_ndarray(value):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m   ndarray = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__array__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m default_types.TENSOR(ndarray.shape, ndarray.dtype)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, custom_nest_protocol.CustomNestProtocol):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:164\u001b[39m, in \u001b[36mKerasTensor.__array__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    165\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor is symbolic: it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms a placeholder for a shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man a dtype. It doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have any actual numerical value. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot convert it to a NumPy array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling layer 'feature_extraction_layer' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'feature_extraction_layer' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=input_layer>\n  • training=None"
     ]
    }
   ],
   "source": [
    "efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n",
    "imagenet_model = create_model(efficientnet_url,\n",
    "                              num_classes=train_data_10_percent.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d26fdd0f-c7df-46a2-8e51-69350b1a6b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x000002775A9F4590> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mhub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m m = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnet_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m m.build([\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m3\u001b[39m])  \u001b[38;5;66;03m# Batch input shape.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:75\u001b[39m, in \u001b[36mSequential.__init__\u001b[39m\u001b[34m(self, layers, trainable, name)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m layers:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrebuild\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_rebuild()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:97\u001b[39m, in \u001b[36mSequential.add\u001b[39m\u001b[34m(self, layer, rebuild)\u001b[39m\n\u001b[32m     95\u001b[39m         layer = origin_layer\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOnly instances of `keras.Layer` can be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33madded to a Sequential model. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_layer_name_unique(layer):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAll layers added to a Sequential model \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshould have unique names. Name \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is already \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe name of a layer in this model. Update the `name` argument \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto pass a unique name.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x000002775A9F4590> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "m = tf.keras.Sequential([\n",
    "    hub.KerasLayer(efficientnet_url)\n",
    "])\n",
    "m.build([None, 224, 224, 3])  # Batch input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abd9f0-f606-4890-beb4-e237237014f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2679ff-0789-477d-911c-fcee606c15e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
